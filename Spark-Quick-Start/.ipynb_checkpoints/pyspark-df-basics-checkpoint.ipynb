{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames Basic app\n",
    "\n",
    "See companion 'pyspark' notebook for setting up pyspark\n",
    "\n",
    "*** This notebook was based on Web posting at https://changhsinlee.com/pyspark-dataframe-basics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic dataframe verbs\n",
    "\n",
    "In R’s dplyr package, Hadley Wickham defined the 5 basic verbs:\n",
    "- select\n",
    "- filter\n",
    "- mutate\n",
    "- summarize\n",
    "- arrange.\n",
    "\n",
    "Here are the equivalents of the 5 basic verbs for Spark dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "I can select a subset of columns. The method select() takes either a list of column names or an unpacked list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "I can filter a subset of rows. The method filter() takes column expressions or SQL expressions. Think of the WHERE clause in SQL queries.\n",
    "\n",
    "####    Filter with a column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter with a SQL expression.\n",
    "Note the double and single quotes as I’m passing a SQL where clause into filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(\"Sex='female'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutate, or creating new columns\n",
    "\n",
    "Create new columns in Spark using .withColumn().\n",
    "Have yet found a convenient way to create multiple columns at once without chaining multiple .withColumn() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize and group by\n",
    "\n",
    "To summarize or aggregate a dataframe, first I need to convert the dataframe to a GroupedData object with groupby(), then call the aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1c579f1f160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "gdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    gdf2\n",
    "    .agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange (sort)\n",
    "\n",
    "Use the .sort() method to sort the dataframes. I haven’t seen a good use case for this in Spark though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and unions\n",
    "\n",
    "There are two ways to combine dataframes — joins and unions.\n",
    "The idea here is the same as joining and unioning tables in SQL.\n",
    "\n",
    "#### Joins\n",
    "\n",
    "For example, I can join the two titanic dataframes by the column PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, ['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also join by conditions, but it creates duplicate column names if the keys have the same name, which is frustrating. For now, the only way I know to avoid this is to pass a list of join keys as in the previous cell. If I want to make nonequi joins, then I need to rename the keys before I join.\n",
    "\n",
    "#### Nonequi joins\n",
    "\n",
    "Here is an example of nonequi join.\n",
    "They can be very slow due to skewed data, but this is one thing that Spark can do that Hive can not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "|          1|    Owen|  male|       0|          1| 22| 7.3|     3|\n",
      "|          1|    Owen|  male|       0|          2| 38|71.3|     1|\n",
      "|          1|    Owen|  male|       0|          3| 26| 7.9|     3|\n",
      "|          1|    Owen|  male|       0|          4| 35|53.1|     1|\n",
      "|          1|    Owen|  male|       0|          5| 35| 8.0|     3|\n",
      "|          2|Florence|female|       1|          2| 38|71.3|     1|\n",
      "|          2|Florence|female|       1|          3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|          4| 35|53.1|     1|\n",
      "|          2|Florence|female|       1|          5| 35| 8.0|     3|\n",
      "|          3|   Laina|female|       1|          3| 26| 7.9|     3|\n",
      "|          3|   Laina|female|       1|          4| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1|          5| 35| 8.0|     3|\n",
      "|          4|    Lily|female|       1|          4| 35|53.1|     1|\n",
      "|          4|    Lily|female|       1|          5| 35| 8.0|     3|\n",
      "|          5| William|  male|       0|          5| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.PassengerId).show() # Note the duplicate col names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unions\n",
    "\n",
    "Union() returns a dataframe from the union of two dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe input and output (I/O)\n",
    "\n",
    "There are two classes pyspark.sql.DataFrameReader and pyspark.sql.DataFrameWriter that handles dataframe I/O. Depending on the configuration, the files may be saved locally, through a Hive metasore, or to a Hadoop file system (HDFS).\n",
    "\n",
    "Common methods on saving dataframes to files include saveAsTable() for Hive tables and saveAsFile() for local or Hadoop file system.\n",
    "\n",
    "I will refer to the documentation for examples on how to read and write dataframes for different formats.\n",
    "\n",
    "- DataFrameReader documentation\n",
    "- DataFrameWriter documentation\n",
    "- Source code for reader and writer\n",
    "\n",
    "### The spark.sql API\n",
    "\n",
    "Many of the operations that I showed can be accessed by writing SQL (Hive) queries in spark.sql(). This is also a convenient way to read Hive tables into Spark dataframes. To make an existing Spark dataframe usable for spark.sql(), I need to register said dataframe as a temporary table.\n",
    "\n",
    "### Temp tables\n",
    "\n",
    "As an example, I can register the two dataframes as temp tables then join them through spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n",
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data persistence: cache() and checkpoint()\n",
    "\n",
    "#### caching\n",
    "\n",
    "Proper caching is the key to high performance Spark. But to be honest, I still don’t have good intuition on when to cache and when not to cache. I do know a rule of thumb that\n",
    "\n",
    "    Cache a dataframe when it is used multiple times in the script.\n",
    "\n",
    "Keep in mind that a dataframe only cached after the first action such as saveAsTable(). If for whatever reason I want to make sure the data is cached before I save the dataframe, then I have to call an action like .count() before I save it.\n",
    "\n",
    "df1.cache()\n",
    "\n",
    "DataFrame[PassengerId: bigint, Name: string, Sex: string, Survived: bigint]\n",
    "\n",
    "This also works as .cache() is an inplace method.\n",
    "\n",
    "df1 = df1.cache()\n",
    "\n",
    "To check if a dataframe is cached, check the storageLevel property.\n",
    "\n",
    "df1.storageLevel\n",
    "\n",
    "StorageLevel(True, True, False, True, 1)\n",
    "\n",
    "To un-cache a dataframe, use unpersist()\n",
    "\n",
    "df1.unpersist()\n",
    "df1.storageLevel\n",
    "\n",
    "StorageLevel(False, False, False, False, 1)\n",
    "\n",
    "There are 4 caching levels that can be fine-tuned with persist(), but I have not seen a use case where fine tuning has been worthwhile. Refer to the persist() documentation for detail.\n",
    "\n",
    "#### Checkpointing\n",
    "\n",
    "I said that sometimes chaining too many union() cause performance problem or even out of memory errors. checkpoint() truncates the execution plan and saves the checkpointed dataframe to a temporary location on the disk.\n",
    "\n",
    "Jacek Laskowski recommends caching before checkpointing so Spark doesn’t have to read in the dataframe from disk after it’s checkpointed.\n",
    "\n",
    "To use checkpoint(), I need to specify the temporary file location to save the datafame to by accessing the sparkContext object from SparkSession.\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"/checkpointdir\") # save to c:/checkpointdir\n",
    "\n",
    "For example, I can join df1 to itself 3 times:\n",
    "\n",
    "df = df1.join(df1, ['PassengerId'])\n",
    "df.join(df1, ['PassengerId']).explain()\n",
    "\n",
    "== Physical Plan ==\n",
    "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#287, Sex#288, Survived#289L, Name#299, Sex#300, Survived#301L]\n",
    "+- *SortMergeJoin [PassengerId#0L], [PassengerId#298L], Inner\n",
    "   :- *Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#287, Sex#288, Survived#289L]\n",
    "   :  +- *SortMergeJoin [PassengerId#0L], [PassengerId#286L], Inner\n",
    "   :     :- *Sort [PassengerId#0L ASC NULLS FIRST], false, 0\n",
    "   :     :  +- Exchange hashpartitioning(PassengerId#0L, 200)\n",
    "   :     :     +- *Filter isnotnull(PassengerId#0L)\n",
    "   :     :        +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L]\n",
    "   :     +- *Sort [PassengerId#286L ASC NULLS FIRST], false, 0\n",
    "   :        +- Exchange hashpartitioning(PassengerId#286L, 200)\n",
    "   :           +- *Filter isnotnull(PassengerId#286L)\n",
    "   :              +- Scan ExistingRDD[PassengerId#286L,Name#287,Sex#288,Survived#289L]\n",
    "   +- *Sort [PassengerId#298L ASC NULLS FIRST], false, 0\n",
    "      +- Exchange hashpartitioning(PassengerId#298L, 200)\n",
    "         +- *Filter isnotnull(PassengerId#298L)\n",
    "            +- Scan ExistingRDD[PassengerId#298L,Name#299,Sex#300,Survived#301L]\n",
    "\n",
    "I can also checkpoint() after the first join to truncate the plan.\n",
    "\n",
    "df = df1.join(df1, ['PassengerId']).checkpoint()\n",
    "df.join(df1, ['PassengerId']).explain()\n",
    "\n",
    "== Physical Plan ==\n",
    "*Project [PassengerId#0L, Name#1, Sex#2, Survived#3L, Name#314, Sex#315, Survived#316L, Name#335, Sex#336, Survived#337L]\n",
    "+- *SortMergeJoin [PassengerId#0L], [PassengerId#334L], Inner\n",
    "   :- *Filter isnotnull(PassengerId#0L)\n",
    "   :  +- Scan ExistingRDD[PassengerId#0L,Name#1,Sex#2,Survived#3L,Name#314,Sex#315,Survived#316L]\n",
    "   +- *Sort [PassengerId#334L ASC NULLS FIRST], false, 0\n",
    "      +- Exchange hashpartitioning(PassengerId#334L, 200)\n",
    "         +- *Filter isnotnull(PassengerId#334L)\n",
    "            +- Scan ExistingRDD[PassengerId#334L,Name#335,Sex#336,Survived#337L]\n",
    "\n",
    "### Partitions and repartition()\n",
    "\n",
    "Another common cause of performance problems for me was having too many partitions. I think the Hadoop world call this the small file problem. A rule of thumb, which I first heard from these slides, is\n",
    "\n",
    "    Keep the partitions to ~128MB.\n",
    "\n",
    "In PySpark, however, there is no way to infer the size of the dataframe partitions. In my experience, as long as the partitions are not 10KB or 10GB but are in the order of MBs, then the partition size shouldn’t be too much of a problem.\n",
    "\n",
    "To check the number of partitions, use .rdd.getNumPartitions()\n",
    "\n",
    "df1.rdd.getNumPartitions()\n",
    "\n",
    "4\n",
    "\n",
    "This dataframe, despite having only 5 rows, has 4 partitions. This is too many. I can repartition to only 1 partition.\n",
    "\n",
    "df1_repartitioned = df1.repartition(1)\n",
    "df1_repartitioned.rdd.getNumPartitions()\n",
    "\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
